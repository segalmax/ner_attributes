{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NER_models.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lto7u-Twr0h",
    "colab_type": "text"
   },
   "source": [
    "**Using Named Entity Recognition techniques to extract attributes from E-Commerce products data**\n",
    "\n",
    "Students:\n",
    "Maxim Segal, id: 317026557\n",
    "Ron Levi, id: 200541456\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWZL00Ykwx5b",
    "colab_type": "text"
   },
   "source": [
    "**Goal**\n",
    "\n",
    "We want to address a problem that is very important and prevalent in different domains of eCommerce: extracting products' attributes and their values from plain textual data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHnIK6z5w3RU",
    "colab_type": "text"
   },
   "source": [
    "**Motivation**\n",
    "\n",
    "Quality extraction of products' attributes and having them saved in a structured manner can significantly improve search results in eCommerce retail system, enrich products catalog and improve the overall customer experience. \n",
    "\n",
    "Any eCommerce player, big as small, seeks to accurately predict and answer the customers' demand for products. This may lead to ways of enriching the offered catalogs.\n",
    "\n",
    "Also, in the today's competitive world between eccomerce players, by precisely satisfying the demand for goods to the customers - the overall well-being of the economy increases.\n",
    "\n",
    "Subsequently, we would like to address the above by building a tool that merges several existing state-of-the-art approaches in the field of Named Entity Recognition and tailor a solution for this specific task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57GBVTvqb01w",
    "colab_type": "text"
   },
   "source": [
    "In the above image we can see that customer query is not met appropriatly. We searched for a white iphone 7 but the results show that the phone model was not recognized from user's query. We aim to recognize the model of the product (in this case- 7) which will assist with search results optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCmFZN75w75T",
    "colab_type": "text"
   },
   "source": [
    "**Proposed Solutions**\n",
    "\n",
    "We want to try achieving viable, quality solution to product attributes extraction: \n",
    "\n",
    "given a plain text descriptions and/or other (unstructured) specifications we aim to extract the needed data in (attribute:value) pairs. As said, it can be utilized for search optimization and also helping decision makers to attain a better understanding of their catalog and make informed buisness decisions. Our pipeline will be as follows:\n",
    "![picture](https://www.nltk.org/images/ie-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_yTYZIGxEm9",
    "colab_type": "text"
   },
   "source": [
    "**Datasets**\n",
    "\n",
    "Initially, for the purpose of this POC and due to products dataset currently unavailable, we will demonstrate several NER approaches in conceptually similar tasks. We will use alternative datasets just to present these techniques. The datasets are:\n",
    "- **CoNLL 2002**:\n",
    "Spanish annotated dataset from NLTK for extracting person/organization/location/misc named entities from spanish corpora. This dataset is composed of list of sentences where each sentence is structured in the standard representation scheme of IOB tags, a tuple in following order: (the token, its Part Of Speach tag, its entity label).\n",
    "Lets see how the annotated sentences data actually look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EnjjYIFETETU",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "outputId": "db7e4c95-841e-4952-dde5-2ccc3d898ddb"
   },
   "source": [
    "import nltk\n",
    "nltk.download('conll2002') # dataset\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "print('This is an example of an annotated sentence: \\n')\n",
    "train_sents[5]\n"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\segal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of an annotated sentence: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('Por', 'SP', 'O'),\n ('su', 'DP', 'O'),\n ('parte', 'NC', 'O'),\n (',', 'Fc', 'O'),\n ('el', 'DA', 'O'),\n ('Abogado', 'NC', 'B-PER'),\n ('General', 'AQ', 'I-PER'),\n ('de', 'SP', 'O'),\n ('Victoria', 'NC', 'B-LOC'),\n (',', 'Fc', 'O'),\n ('Rob', 'NC', 'B-PER'),\n ('Hulls', 'AQ', 'I-PER'),\n (',', 'Fc', 'O'),\n ('indicÃ³', 'VMI', 'O'),\n ('que', 'CS', 'O'),\n ('no', 'RN', 'O'),\n ('hay', 'VAI', 'O'),\n ('nadie', 'PI', 'O'),\n ('que', 'PR', 'O'),\n ('controle', 'VMS', 'O'),\n ('que', 'CS', 'O'),\n ('las', 'DA', 'O'),\n ('informaciones', 'NC', 'O'),\n ('contenidas', 'AQ', 'O'),\n ('en', 'SP', 'O'),\n ('CrimeNet', 'NC', 'B-MISC'),\n ('son', 'VSI', 'O'),\n ('veraces', 'AQ', 'O'),\n ('.', 'Fp', 'O')]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYUSIGOryL46",
    "colab_type": "text"
   },
   "source": [
    "- **CoNLL 2003**: \n",
    "\n",
    "The CoNLL-2003 is an English named entity dataset that includes 17 label types and is originated from a shared task of annotating the Reuters Corpus. Its layout is similar to the previously described CoNLL 2002 dataset with an added column specifying the indices of each sentence. Here we show words distribution across the dataset and the tabular display of a single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\FK_branch\\ut_pr_01\\src\\ner_attributes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print( os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fx-4dyw02Rfo",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "3171d98e-2263-40f8-b52a-90bbd072e4f2"
   },
   "source": [
    "# ! pip -q install git+https://www.github.com/keras-team/keras-contrib.git sklearn-crfsuite\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# After hyperparams optimization\n",
    "BATCH_SIZE = 512  # Number of examples used in each iteration\n",
    "EPOCHS = 5  # Number of passes through entire dataset\n",
    "MAX_LEN = 80  # Max length of review (in words)\n",
    "EMBEDDING = 40  # Dimension of word embedding vector\n",
    "data = pd.read_csv('computer_with_indexes_200720-183555.csv', encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "\n",
    "print(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n",
    "\n",
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words)\n",
    "print(\"Number of words in the dataset: \", n_words)\n",
    "\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "print(\"Tags:\", tags)\n",
    "n_tags = len(tags)\n",
    "print(\"Number of Labels: \", n_tags)\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \"\"\"Class to Get the sentence in this format:\n",
    "    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]\"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Args:\n",
    "            data is the pandas.DataFrame which contains the above dataset\"\"\"\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "   \n",
    "getter = SentenceGetter(data)\n",
    "# Get all the sentences\n",
    "sentences = getter.sentences\n",
    "\n",
    "# Plot sentence by length\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.title('Token per sentence')\n",
    "plt.xlabel('Len (number of token)')\n",
    "plt.ylabel('# samples')\n",
    "plt.show()\n",
    "\n",
    "print(\"What the dataset looks like: \")\n",
    "# Show the first 10 rows\n",
    "data.head(24)"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xru6isAgx8y",
    "colab_type": "text"
   },
   "source": [
    "**Algorithms and Frameworks**\n",
    "\n",
    "Our research included multiple machine learning classifiers including CRF and RNNs combined with CRF. \n",
    "We will employ deep learning methods from Tensorflow, Keras, spaCy along with traditional machine learning algorithms from scikit-learn and sklearn-crfsuite for entities recognition in the products attributes extraction context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvRel23VOHHf",
    "colab_type": "text"
   },
   "source": [
    "**Proof Of Concept**\n",
    "\n",
    "We want to show two approaches to solve our problem:\n",
    "  1. Conditional random fields (CRF) - a class of discriminative models suited to prediction tasks such as ours, where contextual information or state of the neighbors affect the current prediction. \n",
    "  2. Bi-directional LSTM-CRF model - a neural network that benefits from word and character level information and dependencies across adjacent labels which is also useful for textual data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uUjmpeETgLw",
    "colab_type": "text"
   },
   "source": [
    "**Conditional random fields (CRF):**\n",
    "\n",
    "In this example we show a usage of CRF classifier on the person-organization-location dataset.\n",
    "We first preprocess our data by assigning features to each of the tokens of the sentences and provide the information in that form to the CRF model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h-utbXFuvU4Z",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "outputId": "05586774-543d-4bc8-d601-8ec4f6bcb086"
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('conll2002') # dataset\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]    \n",
    "\n",
    "!pip install sklearn_crfsuite\n",
    "# employ crf model\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "def crf_pipeline(X_train, y_train, X_test, y_test):\n",
    "  crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=200, all_possible_transitions=True)\n",
    "  crf.fit(X_train, y_train)\n",
    "  labels = list(crf.classes_)\n",
    "  y_pred = crf.predict(X_test)\n",
    "  print(metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels))\n",
    "  print(metrics.flat_classification_report(y_test, y_pred, labels=labels, digits=3))\n",
    "\n",
    "crf_pipeline(X_train, y_train, X_test, y_test)  \n"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\segal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\segal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn_crfsuite in c:\\users\\segal\\anaconda3\\envs\\project_ds\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: tabulate in c:\\users\\segal\\anaconda3\\envs\\project_ds\\lib\\site-packages (from sklearn_crfsuite) (0.8.7)\n",
      "Requirement already satisfied: six in c:\\users\\segal\\anaconda3\\envs\\project_ds\\lib\\site-packages (from sklearn_crfsuite) (1.15.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\segal\\anaconda3\\envs\\project_ds\\lib\\site-packages (from sklearn_crfsuite) (4.46.1)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\segal\\anaconda3\\envs\\project_ds\\lib\\site-packages (from sklearn_crfsuite) (0.9.7)\n",
      "0.9713896997078182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.813     0.787     0.800      1084\n",
      "           O      0.993     0.997     0.995     45355\n",
      "       B-ORG      0.813     0.834     0.823      1400\n",
      "       B-PER      0.844     0.888     0.865       735\n",
      "       I-PER      0.878     0.940     0.908       634\n",
      "      B-MISC      0.717     0.560     0.629       339\n",
      "       I-ORG      0.860     0.789     0.823      1104\n",
      "       I-LOC      0.697     0.643     0.669       325\n",
      "      I-MISC      0.725     0.621     0.669       557\n",
      "\n",
      "    accuracy                          0.972     51533\n",
      "   macro avg      0.815     0.784     0.798     51533\n",
      "weighted avg      0.971     0.972     0.971     51533\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdAauZ2tIL41",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "We can see that the scores are reasonable, although we may change our  algorithms if we will discover it is not suitable for the nature of our problem. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp7M5HcaUrrL",
    "colab_type": "text"
   },
   "source": [
    "**Bi-directional LSTM-CRF model**\n",
    "\n",
    "In this approach, we use words embeddings made from the textual data and feed it to a neural net with bidirectional Conditional Random Field layer.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3JWdN4nfR_7p",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "fa31d01e-306b-4a2b-8855-477cadbc3b5e"
   },
   "source": [
    "\n",
    "######## preprocessing ##########\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"PAD\"] = 0 # Padding\n",
    "word2idx[\"UNK\"] = 1 # Unknown words\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert each sentence from list of Token to list of word_index\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# Convert Tag/Label to tag_index\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "# Padding each sentence to have the same lenght\n",
    "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "# One-Hot encode\n",
    "y = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "########### Bidirectional-LSTM-CRF model ############\n",
    "\n",
    "! pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "# Model definition\n",
    "input = Input(shape=(MAX_LEN,))\n",
    "model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
    "                  input_length=MAX_LEN)(input)  # default: 20-dim embedding\n",
    "model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                           recurrent_dropout=0.1))(model)  # variational biLSTM\n",
    "model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
    "crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Model training \n",
    "history = model.fit(X_tr, np.array(y_tr), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1, verbose=2)\n",
    "\n",
    "# Evalualtion\n",
    "pred_cat = model.predict(X_te)\n",
    "pred = np.argmax(pred_cat, axis=-1)\n",
    "y_te_true = np.argmax(y_te, -1)\n",
    "\n",
    "!pip install sklearn_crfsuite\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "# Convert the index to tag\n",
    "pred_tag = [[idx2tag[i] for i in row] for row in pred]\n",
    "y_te_true_tag = [[idx2tag[i] for i in row] for row in y_te_true] \n",
    "\n",
    "report = flat_classification_report(y_pred=pred_tag, y_true=y_te_true_tag)\n",
    "print(report)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to c:\\users\\segal\\appdata\\local\\temp\\pip-req-build-2ciaxiwj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git 'C:\\Users\\segal\\AppData\\Local\\Temp\\pip-req-build-2ciaxiwj'\n",
      "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git clone -q https://www.github.com/keras-team/keras-contrib.git 'C:\\Users\\segal\\AppData\\Local\\Temp\\pip-req-build-2ciaxiwj'\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_contrib'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-13-19fc34b05fdf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mModel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLSTM\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mEmbedding\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDense\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTimeDistributed\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDropout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mBidirectional\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras_contrib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mCRF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;31m# Model definition\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'keras_contrib'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZAQgeFoLehI",
    "colab_type": "text"
   },
   "source": [
    "Even though the Bi-LSTM-CRF results on the CoNLL 2003 are dubious, according to the literature the scores are usually higher and we will understand how to achieve better results going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7UAmdmyYquR",
    "colab_type": "text"
   },
   "source": [
    "**Reporting**\n",
    "\n",
    "In our task we want to consider the F1 score while giving emphasis on precision.\n",
    "We want to build our data set from an exisiting production eCommerce database. Currently we have tagged products data with several attributes and need to translate it to the needed format (token, POS, attribute label). After that we want to experiment with the mentioned and other models in an attempt to reach the highest possible scores. \n",
    "\n",
    "We would like to achieve a model in which the classifying labeling will run automatically preferably without expert human tagging.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKQXHYJajUq1",
    "colab_type": "text"
   },
   "source": [
    "**Summary**\n",
    "\n",
    "Eventually we want to build and present the training, test and validation datasets that will consist of real online products unstructured data (i.e. description, names and specifications).\n",
    "\n",
    "Out of the presented algoritms, for a similar task to ours, CRF currently seems to be able to achieve best results in terms of recall and precision. But nethertheless we want to adjust Bi-LSTM-CRF to our datasets and try to explore and combine other advanced techniques to improve quality of attributes labeling in terms of suitable conventional scoring metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cT-MthjYtXTv",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "33216fff-851a-41b5-ce02-0f262fe31db9"
   },
   "source": [
    "import os\n",
    "os.getcwd()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}